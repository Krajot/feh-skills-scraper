{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c030f67-f91b-4611-ae8f-d3149e1a63b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37b23aec-8424-4ea1-bb10-7ed4f7b413e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableScraper:\n",
    "    def __init__(self, delay=1):\n",
    "        \"\"\"\n",
    "        Initialize the scraper with optional delay between requests\n",
    "        \"\"\"\n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        })\n",
    "        self.delay = delay\n",
    "    \n",
    "    def scrape_tables(self, url, table_selector=None, save_to_csv=False, filename=None):\n",
    "        \"\"\"\n",
    "        Scrape all tables from a webpage\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL of the webpage\n",
    "            table_selector (str): CSS selector for specific tables (optional)\n",
    "            save_to_csv (bool): Whether to save tables as CSV files\n",
    "            filename (str): Base filename for CSV files\n",
    "        \n",
    "        Returns:\n",
    "            list: List of pandas DataFrames containing table data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Fetching data from: {url}\")\n",
    "            response = self.session.get(url)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find tables\n",
    "            if table_selector:\n",
    "                tables = soup.select(table_selector)\n",
    "            else:\n",
    "                tables = soup.find_all('table')\n",
    "            \n",
    "            if not tables:\n",
    "                print(\"No tables found on the page\")\n",
    "                return []\n",
    "            \n",
    "            print(f\"Found {len(tables)} table(s)\")\n",
    "            \n",
    "            dataframes = []\n",
    "            \n",
    "            for i, table in enumerate(tables):\n",
    "                print(f\"Processing table {i+1}/{len(tables)}\")\n",
    "                \n",
    "                # Extract table data\n",
    "                df = self._parse_table(table)\n",
    "                \n",
    "                if df is not None and not df.empty:\n",
    "                    dataframes.append(df)\n",
    "                    \n",
    "                    # Save to CSV if requested\n",
    "                    if save_to_csv:\n",
    "                        csv_filename = f\"{filename or 'table'}_{i+1}.csv\"\n",
    "                        df.to_csv(csv_filename, index=False)\n",
    "                        print(f\"Saved table {i+1} to {csv_filename}\")\n",
    "                \n",
    "                # Add delay between processing\n",
    "                if i < len(tables) - 1:\n",
    "                    time.sleep(self.delay)\n",
    "            \n",
    "            return dataframes\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching the webpage: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def _parse_table(self, table):\n",
    "        \"\"\"\n",
    "        Parse a BeautifulSoup table element into a pandas DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Find all rows\n",
    "            rows = table.find_all('tr')\n",
    "            if not rows:\n",
    "                return None\n",
    "            \n",
    "            # Extract headers\n",
    "            headers = []\n",
    "            first_row = rows[0]\n",
    "            header_cells = first_row.find_all(['th', 'td'])\n",
    "            \n",
    "            for cell in header_cells:\n",
    "                headers.append(cell.get_text(strip=True))\n",
    "            \n",
    "            # If no proper headers found, create generic ones\n",
    "            if not headers or all(not h for h in headers):\n",
    "                headers = [f\"Column_{i+1}\" for i in range(len(header_cells))]\n",
    "            \n",
    "            # Extract data rows\n",
    "            data = []\n",
    "            start_row = 1 if first_row.find('th') else 0\n",
    "            \n",
    "            for row in rows[start_row:]:\n",
    "                cells = row.find_all(['td', 'th'])\n",
    "                row_data = []\n",
    "                \n",
    "                for cell in cells:\n",
    "                    cell_text = cell.get_text(strip=True)\n",
    "                    row_data.append(cell_text)\n",
    "                \n",
    "                # Only add rows that have data\n",
    "                if row_data and any(cell.strip() for cell in row_data):\n",
    "                    # Pad row to match header length\n",
    "                    while len(row_data) < len(headers):\n",
    "                        row_data.append('')\n",
    "                    data.append(row_data[:len(headers)])\n",
    "            \n",
    "            if not data:\n",
    "                return None\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(data, columns=headers)\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing table: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_specific_table(self, url, table_index=0):\n",
    "        \"\"\"\n",
    "        Scrape a specific table by index\n",
    "        \"\"\"\n",
    "        tables = self.scrape_tables(url)\n",
    "        if tables and len(tables) > table_index:\n",
    "            return tables[table_index]\n",
    "        return None\n",
    "    \n",
    "    def scrape_tables_with_pandas(self, url):\n",
    "        \"\"\"\n",
    "        Alternative method using pandas read_html (simpler but less control)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Using pandas to scrape tables from: {url}\")\n",
    "            tables = pd.read_html(url)\n",
    "            print(f\"Found {len(tables)} table(s) using pandas\")\n",
    "            return tables\n",
    "        except Exception as e:\n",
    "            print(f\"Error using pandas read_html: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da068017-56f0-46b8-941b-d6ebfeb5bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage functions\n",
    "def example_basic_scraping():\n",
    "    \"\"\"\n",
    "    Basic example of scraping tables from a webpage\n",
    "    \"\"\"\n",
    "    scraper = TableScraper()\n",
    "    \n",
    "    # Example URL (replace with your target URL)\n",
    "    url = \"https://feheroes.fandom.com/wiki/Sacred_Seals#List_of_Sacred_Seals\"\n",
    "    \n",
    "    # Scrape all tables\n",
    "    tables = scraper.scrape_tables(url, save_to_csv=True, filename=\"sacred_seals\")\n",
    "    \n",
    "    # Display first few rows of each table\n",
    "    for i, df in enumerate(tables):\n",
    "        print(f\"\\n--- Table {i+1} ---\")\n",
    "        print(df.head())\n",
    "        print(f\"Shape: {df.shape}\")\n",
    "\n",
    "def example_specific_table():\n",
    "    \"\"\"\n",
    "    Example of scraping a specific table with CSS selector\n",
    "    \"\"\"\n",
    "    scraper = TableScraper()\n",
    "    \n",
    "    url = \"https://example.com/data-page\"\n",
    "    \n",
    "    # Scrape tables with specific CSS selector\n",
    "    tables = scraper.scrape_tables(\n",
    "        url, \n",
    "        table_selector=\"table.data-table\",  # CSS selector for specific tables\n",
    "        save_to_csv=True,\n",
    "        filename=\"specific_tables\"\n",
    "    )\n",
    "    \n",
    "    return tables\n",
    "\n",
    "def example_pandas_method():\n",
    "    \"\"\"\n",
    "    Example using pandas read_html method\n",
    "    \"\"\"\n",
    "    scraper = TableScraper()\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/List_of_countries_by_GDP_(nominal)\"\n",
    "    \n",
    "    # Using pandas method (simpler but less control)\n",
    "    tables = scraper.scrape_tables_with_pandas(url)\n",
    "    \n",
    "    if tables:\n",
    "        # Save first table\n",
    "        tables[0].to_csv(\"gdp_data.csv\", index=False)\n",
    "        print(\"GDP data saved to gdp_data.csv\")\n",
    "    \n",
    "    return tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed167021-710d-45e5-ab64-174e78e09e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web Table Scraper\n",
      "==================================================\n",
      "1. Basic scraping example\n",
      "2. Specific table with CSS selector\n",
      "3. Using pandas read_html method\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter choice (1-3):  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data from: https://feheroes.fandom.com/wiki/Sacred_Seals#List_of_Sacred_Seals\n",
      "Found 2 table(s)\n",
      "Processing table 1/2\n",
      "Saved table 1 to sacred_seals_1.csv\n",
      "Processing table 2/2\n",
      "Saved table 2 to sacred_seals_2.csv\n",
      "\n",
      "--- Table 1 ---\n",
      "  Icon          Name                                        Description   SP  \\\n",
      "0       Aerobatics 1  If unit's HP = 100%, unit can move to a space ...   60   \n",
      "1       Aerobatics 2  If unit's HP ≥ 50%, unit can move to a space a...  120   \n",
      "2       Aerobatics 3  Unit can move to a space adjacent to any infan...  240   \n",
      "3       Air Orders 1  At start of turn, if unit's HP = 100%, grants ...   60   \n",
      "4       Air Orders 2  At start of turn, if unit's HP ≥ 50%, grants t...  120   \n",
      "\n",
      "  Badge Color  \n",
      "0              \n",
      "1              \n",
      "2              \n",
      "3              \n",
      "4              \n",
      "Shape: (926, 5)\n",
      "\n",
      "--- Table 2 ---\n",
      "                                     veSkills\n",
      "0  WeaponsAssistsSpecialsPassivesSacred Seals\n",
      "Shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Web Table Scraper\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Choose which example to run\n",
    "    print(\"1. Basic scraping example\")\n",
    "    print(\"2. Specific table with CSS selector\")\n",
    "    print(\"3. Using pandas read_html method\")\n",
    "    \n",
    "    choice = input(\"Enter choice (1-3): \").strip()\n",
    "    \n",
    "    if choice == \"1\":\n",
    "        example_basic_scraping()\n",
    "    elif choice == \"2\":\n",
    "        example_specific_table()\n",
    "    elif choice == \"3\":\n",
    "        example_pandas_method()\n",
    "    else:\n",
    "        print(\"Running basic example...\")\n",
    "        example_basic_scraping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d7c92-09be-45a0-b461-247b0dc369b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
